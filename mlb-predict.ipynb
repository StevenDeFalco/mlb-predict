{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd8593b",
   "metadata": {},
   "source": [
    "# MLB-Predict\n",
    "\n",
    "### In this notebook, I will...\n",
    "* import code from data.py\n",
    "* instantiate an intances of the LeagueStats class \n",
    "* collect a range of historical game data from that team\n",
    "* save the historical data to an .xlsx file\n",
    "* load the historical data into this notebook\n",
    "* prepare the data for training (i.e. strip unnecessary features)\n",
    "* divide data into testing and training sets\n",
    "* fine tune an existing gradient boosting framework\n",
    "* tune hyperparameters to get best model and then save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb226f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data import LeagueStats, TeamStats\n",
    "\n",
    "mlb = LeagueStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b20bb",
   "metadata": {},
   "source": [
    "## MLB Data Retrieval Example\n",
    "\n",
    "This shows how I would retrieve data manually using my LeagueStats class. In reality, I use the data_retriever.py script to collect massive amounts of data (seasons at a time) and have it automatically saved to disk in an organized format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlb.get_data(start_date=\"04/01/2023\", end_date=\"08/15/2023\", file_path=\"data/seasons/2023.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4200657",
   "metadata": {},
   "source": [
    "## Example Data Preparation and Model Training\n",
    "\n",
    "The code below shows the steps for preparing data for training and how I would train a model. It is not fully executable due to missing file paths and data etc. This is just used to show the steps that I take and which I eventually consolidate in the prepare_data function. Below the definition of prepare_data begins the actual training for various models including the discontinued  mlb3year model and my new mlb4year model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88373b25",
   "metadata": {},
   "source": [
    "## Merging and loading data\n",
    "Load and merge season data from each xlsx file into a single xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# path to data sheets to combine data from\n",
    "directory = '<data_sheets_folder>'\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# iterate through directory to combine data\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        path = os.path.join(directory, filename)\n",
    "        df = pd.read_excel(path)\n",
    "        data = pd.concat([data, df], ignore_index=True)\n",
    "        \n",
    "data.to_excel('<combined_data>.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4072f27",
   "metadata": {},
   "source": [
    "#### *Loading and processing data* \n",
    "Load data from the master .xlsx file into a data frame. Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('<combined_data>.xlsx')\n",
    "\n",
    "# remove the game-id, date, home/away team features\n",
    "data.drop(columns=['game-id', 'date', 'home-team', 'away-team'], inplace=True)\n",
    "\n",
    "# drops rows with missing labels\n",
    "data = data.dropna(subset=['did-home-win'])\n",
    "\n",
    "# convert 'did-home-win' labels to binary values\n",
    "data['did-home-win'] = data['did-home-win'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59e2be",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "#### *Experminetal Optimization*\n",
    "Rearrange the order of the features to attempt to optimize the model\n",
    "\n",
    "* Order 1: Place most important features first with each home team statistic immediately followed by the away's counter part. Allows for many meaningful comparisons between adjacent features\n",
    "* Order 2: Place most import features first with all of the home teams statistics appearing before any of the away team's statistics. Stats for each team are in the same order just completely separated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "order1 = [\n",
    "    \"did-home-win\",\n",
    "    \"home-win-percentage\", \"away-win-percentage\",\n",
    "    \"home-starter-season-era\", \"away-starter-season-era\",\n",
    "    \"home-starter-season-win-percentage\", \"away-starter-season-win-percentage\",\n",
    "    \"home-top5-hr-avg\", \"away-top5-hr-avg\",\n",
    "    \"home-last10-avg-runs\", \"away-last10-avg-runs\",\n",
    "    \"home-last10-avg-ops\", \"away-last10-avg-ops\",\n",
    "    \"home-starter-season-whip\", \"away-starter-season-whip\",\n",
    "    \"home-top5-rbi-avg\", \"away-top5-rbi-avg\",\n",
    "    \"home-last10-avg-runs-allowed\", \"away-last10-avg-runs-allowed\",\n",
    "    \"home-starter-season-avg\", \"away-starter-season-avg\",\n",
    "    \"home-top5-batting-avg\", \"away-top5-batting-avg\",\n",
    "    \"home-starter-season-strike-percentage\", \"away-starter-season-strike-percentage\",\n",
    "    \"home-last10-avg-hits\", \"away-last10-avg-hits\",\n",
    "    \"home-last10-avg-hits-allowed\", \"away-last10-avg-hits-allowed\",\n",
    "    \"home-last10-avg-obp\", \"away-last10-avg-obp\",\n",
    "    \"home-last10-avg-avg\", \"away-last10-avg-avg\",\n",
    "    \"home-last10-avg-rbi\", \"away-last10-avg-rbi\",\n",
    "    \"home-starter-season-runs-per9\", \"away-starter-season-runs-per9\",\n",
    "    \"home-top5-stolenBases-avg\", \"away-top5-stolenBases-avg\",\n",
    "    \"home-top5-totalBases-avg\", \"away-top5-totalBases-avg\",\n",
    "    \"home-last10-avg-strikeouts\", \"away-last10-avg-strikeouts\",\n",
    "    \"home-starter-career-era\", \"away-starter-career-era\",\n",
    "]\n",
    "\n",
    "order2 = [\n",
    "    \"did-home-win\",\n",
    "    \"home-win-percentage\", \"home-starter-season-era\",\n",
    "    \"home-starter-season-win-percentage\", \"home-top5-hr-avg\",\n",
    "    \"home-last10-avg-runs\", \"home-last10-avg-ops\",\n",
    "    \"home-starter-season-whip\", \"home-top5-rbi-avg\",\n",
    "    \"home-last10-avg-runs-allowed\", \"home-starter-season-avg\",\n",
    "    \"home-top5-batting-avg\", \"home-starter-season-strike-percentage\",\n",
    "    \"home-last10-avg-hits\", \"home-last10-avg-hits-allowed\",\n",
    "    \"home-last10-avg-obp\", \"home-last10-avg-avg\",\n",
    "    \"home-last10-avg-rbi\", \"home-starter-season-runs-per9\",\n",
    "    \"home-top5-stolenBases-avg\", \"home-top5-totalBases-avg\",\n",
    "    \"home-last10-avg-strikeouts\", \"home-starter-career-era\",\n",
    "    \"away-win-percentage\", \"away-starter-season-era\",\n",
    "    \"away-starter-season-win-percentage\", \"away-top5-hr-avg\",\n",
    "    \"away-last10-avg-runs\", \"away-last10-avg-ops\",\n",
    "    \"away-starter-season-whip\", \"away-top5-rbi-avg\",\n",
    "    \"away-last10-avg-runs-allowed\", \"away-starter-season-avg\",\n",
    "    \"away-top5-batting-avg\", \"away-starter-season-strike-percentage\",\n",
    "    \"away-last10-avg-hits\", \"away-last10-avg-hits-allowed\",\n",
    "    \"away-last10-avg-obp\", \"away-last10-avg-avg\",\n",
    "    \"away-last10-avg-rbi\", \"away-starter-season-runs-per9\",\n",
    "    \"away-top5-stolenBases-avg\", \"away-top5-totalBases-avg\",\n",
    "    \"away-last10-avg-strikeouts\", \"away-starter-career-era\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b98948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the columns\n",
    "data = data[order2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964ee8b",
   "metadata": {},
   "source": [
    "#### *Drop rows with missing labels*\n",
    "Drops all rows that are missing more than THRESHOLD features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant representing the amount of features that must missing for a row to be excluded/removed\n",
    "THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops rows with too many missing features\n",
    "data = data.dropna(thresh=df.shape[1] - THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb71357",
   "metadata": {},
   "source": [
    "#### *Min-Max Feature Normalization*\n",
    "Normalize the numeric features to a scale of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2eaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# apply min-max normalization to selected features\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "scaler_path = 'models/scalers/'\n",
    "with open(scaler_path + '<scaler_name>.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63a828",
   "metadata": {},
   "source": [
    "#### *Randomize data order, Split training and testing data*\n",
    "Ensures that training and testing data aren't chronologically grouped. Thus, will treat each game more independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# randomize the order of the rows in the dataframe\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# separate labels from the features\n",
    "features = data.drop('did-home-win', axis=1).values\n",
    "labels = data['did-home-win'].values\n",
    "\n",
    "# verify shapes features and labels\n",
    "print(\"Features shape: \", features.shape)\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "\n",
    "indices = list(range(len(features)))\n",
    "split_index = int(0.85 * len(features))\n",
    "\n",
    "train_indices = indices[:split_index]\n",
    "test_indices  = indices[split_index:]\n",
    "\n",
    "x_train = features[train_indices]\n",
    "x_test  = features[btest_indices ]\n",
    "y_train = labels[train_indices]\n",
    "y_test  = labels[test_indices ]\n",
    "\n",
    "# verify shapes of training/testing sets\n",
    "print(\"Training set shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Testing set shape: \" , x_test.shape,  y_test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cd447",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "Using LightGBM model and defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658103d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# model parameters \n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'accuracy',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a LightGBM Dataset with training features and labels\n",
    "train_data = lgb.Dataset(x_train, label=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ffb70",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(params, mlb_train_data, num_boost_round=1000)\n",
    "model.save_model('<model_name>.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee266f1",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "Here I would test the models' accuracy on its testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ad9d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edac4c5",
   "metadata": {},
   "source": [
    "## End of Example/Demo Code\n",
    "\n",
    "Everything above is demo code that is incomplete and is meant to simply show the data prep. / training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce8386",
   "metadata": {},
   "source": [
    "## Generalized Data Preparation using `prepare_data()`\n",
    "\n",
    "Below is the definition of the `prepare_data` function which generalizes that code which was shown above. Below the definition begins the actual training of the discontinued mlb3year model and all models created since then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd        \n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def prepare_data(data_dirs, model_name, order=order2, save_dir=None, missing_data_threshold=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_dirs: list of paths to folders with the .xlsx data sheets\n",
    "        model_name: name that model should be saved as\n",
    "        order: order of the data features used for training \n",
    "        save_dir: file_path to save merged .xlsx sheet to if desired\n",
    "            -> must end in .xlsx and be a valid (existing) file path\n",
    "        missing_data_threshold: max number of acceptable missing features in training sample\n",
    "    \n",
    "    Returns:\n",
    "        x_train, x_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    # iterate through mlb data directory to retrieve each file\n",
    "    for dir in data_dirs:\n",
    "        for filename in os.listdir(dir):\n",
    "            if filename.endswith('.xlsx'):\n",
    "                path = os.path.join(dir, filename)\n",
    "                d = pd.read_excel(path)\n",
    "                df = pd.concat([df, d], ignore_index=True)\n",
    "    if save_dir:\n",
    "        df.to_excel(save_dir)\n",
    "    \n",
    "    # remove the game-id, date, home/away team features\n",
    "    df.drop(columns=['game-id', 'date', 'home-team', 'away-team'], inplace=True)\n",
    "    # drops rows with missing labels\n",
    "    df = df.dropna(subset=['did-home-win'])\n",
    "    # convert 'did-home-win' labels to binary values\n",
    "    df['did-home-win'] = df['did-home-win'].astype(int)\n",
    "    # randomize the order of the rows in the dataframe\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # change feature order of df\n",
    "    df = df[order]\n",
    "    # drop samples missing more than THRESH values\n",
    "    df = df.dropna(thresh=df.shape[1] - missing_data_threshold)\n",
    "    labels = df['did-home-win'].values\n",
    "    df = df.drop(columns=['did-home-win'])\n",
    "    scaler = MinMaxScaler()\n",
    "    # apply min-max normalization to features\n",
    "    df = scaler.fit_transform(df)\n",
    "    scaler_path = 'models/scalers/'\n",
    "    with open(scaler_path + model_name + '_scaler.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    features = df\n",
    "    # verify shapes features and labels\n",
    "    print(\"Features shape: \", df.shape)\n",
    "    print(\"Labels shape: \", df.shape)\n",
    "    indices = list(range(len(features)))\n",
    "    split_index = int(0.85 * len(features))\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices  = indices[split_index:]\n",
    "    x_train = features[train_indices]\n",
    "    x_test  = features[test_indices ]\n",
    "    y_train = labels[train_indices]\n",
    "    y_test  = labels[test_indices ]\n",
    "    # verify shapes of training/testing sets\n",
    "    print(\"Training set shape: \", x_train.shape, y_train.shape)\n",
    "    print(\"Testing set shape: \" , x_test.shape,  y_test.shape )\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21fab2",
   "metadata": {},
   "source": [
    "## MLB 3 Year (DISCONTINUED)\n",
    "Using the data_retrieval.py script, I collected game data from 2021-2023 seasons in the background and I'm going to pull that data and use it to train a model. The data is from every MLB game during these 3 season (2023 cutoff is 07/09). \n",
    "\n",
    "#### *Note*: mlb3year model uses my original data blend which included some ELO statistics that I have decided to move away from; therefore, this model is not for use any more and would not be able to be retrained and predicted on due to new code in data.py etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a721237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = ['data/seasons/2021', 'data/seasons/2022', 'data/seasons/2023']\n",
    "x_train, x_test, y_train, y_test = prepare_data(data_dirs=data, model_name=\"mlb3year\", order=order2, missing_data_threshold=10)\n",
    "\n",
    "# model parameters \n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'accuracy',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "}\n",
    "\n",
    "data = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "model = lgb.train(params, data, num_boost_round=1000)\n",
    "model.save_model('models/mlb3year.txt')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3bc64",
   "metadata": {},
   "source": [
    "## Testing different feature orders\n",
    "In past models, I've used \"order1\" which creates direct comparisons between adjacent features by placing a home team's statistic directly next to the away team's same stat. Here I will try a different order, primarily the order where I place all of one team's statistics first and then the other team's after. Below I will use the same data as ***mlb3year***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = ['data/seasons/2021', 'data/seasons/2022', 'data/seasons/2023']\n",
    "x_train, x_test, y_train, y_test = prepare_data(\n",
    "    data_dirs=data, \n",
    "    model_name=\"mlb3year_test\", \n",
    "    order=order2, \n",
    "    missing_data_threshold=10\n",
    ")\n",
    "\n",
    "# model parameters \n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'accuracy',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "}\n",
    "\n",
    "data = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "model = lgb.train(params, data, num_boost_round=1000)\n",
    "model.save_model('models/mlb3year_test.txt')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1971418",
   "metadata": {},
   "source": [
    "Return from code above...\n",
    "\n",
    "`Features shape:  (5975, 37)`\n",
    "\n",
    "`Labels shape:  (5975, 37)`\n",
    "\n",
    "`Training set shape:  (5078, 36) (5078,)`\n",
    "\n",
    "`Testing set shape:  (897, 36) (897,)`\n",
    "\n",
    "`[LightGBM] [Info] Number of positive: 2727, number of negative: 2351`\n",
    "\n",
    "`[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000804 seconds.`\n",
    "\n",
    "`You can set force_col_wise=true to remove the overhead.`\n",
    "\n",
    "`[LightGBM] [Info] Total Bins 8551`\n",
    "\n",
    "`[LightGBM] [Info] Number of data points in the train set: 5078, number of used features: 36`\n",
    "\n",
    "`[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537022 -> initscore=0.148361`\n",
    "\n",
    "`[LightGBM] [Info] Start training from score 0.148361`\n",
    "\n",
    "`Accuracy: 0.6432552954292085`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b829b",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "After training the model a couple times with \"order2\", it appears that the order doesn't make a significant impact on the model's testing accuracy. If anything, however, I've noticed that the training accuracies were on average a little bit higher. Moving forward, I will use order2 as the default feature order. \n",
    "\n",
    "Additionally, I will retrain the official mlb3year model with order2, so that the saved model is up to date.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc495f34",
   "metadata": {},
   "source": [
    "## MLB 4 Year\n",
    "\n",
    "This model is the first model to use my new data blend that *does not* longer include ELO statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc9bc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = ['data/seasons/2020', 'data/seasons/2021', \n",
    "        'data/seasons/2022', 'data/seasons/2023']\n",
    "x_train, x_test, y_train, y_test = prepare_data(\n",
    "    data_dirs=data, \n",
    "    model_name=\"mlb4year\", \n",
    "    order=order2, \n",
    "    missing_data_threshold=10\n",
    ")\n",
    "\n",
    "# model parameters \n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'accuracy',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 128,\n",
    "    'learning_rate': 0.005,\n",
    "    'tree_learner': 'serial',\n",
    "    'min_data_in_leaf': 20,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'scale_pos_weight': 1.0,\n",
    "}\n",
    "\n",
    "data = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "model = lgb.train(params, data, num_boost_round=500)\n",
    "model.save_model('models/mlb4year.txt')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fed0e3",
   "metadata": {},
   "source": [
    "Return from code above...\n",
    "\n",
    "`Features shape:  (7406, 44)`\n",
    "\n",
    "`Labels shape:  (7406, 44)`\n",
    "\n",
    "`Training set shape:  (6295, 44) (6295,)`\n",
    "\n",
    "`Testing set shape:  (1111, 44) (1111,)`\n",
    "\n",
    "`[LightGBM] [Info] Number of positive: 3355, number of negative: 2940`\n",
    "\n",
    "`[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000666 seconds.`\n",
    "\n",
    "`You can set force_col_wise=true to remove the overhead.`\n",
    "\n",
    "`[LightGBM] [Info] Total Bins 8318`\n",
    "\n",
    "`[LightGBM] [Info] Number of data points in the train set: 6295, number of used features: 44`\n",
    "\n",
    "`[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.532963 -> initscore=0.132042`\n",
    "\n",
    "`[LightGBM] [Info] Start training from score 0.132042`\n",
    "\n",
    "`Accuracy: 0.6552655265526552`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77441068",
   "metadata": {},
   "source": [
    "## Results of MLB 4 Year \n",
    "\n",
    "I am pleased to see that the accuracy has not worsened as a result of my transition away from ELO statistics. My new blend of data features in each sample includes more features that attempt to measure a team's current and season-long momentum and this was meant to supplement the ELO statistics. \n",
    "\n",
    "Additionally, I spent more time tuning the model's hyper parameters and this resulted in a more consistent and higher accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
